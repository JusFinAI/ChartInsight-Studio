# DataPipeline 프로젝트 로드맵

**문서 버전**: 2.0  
**최종 업데이트**: 2025-10-31  
**작성자**: Gemini (Project Supervisor) + cursor.ai inspector  
**통합 문서**: Phase 1-4 계획 및 실행 이력 통합 완료

---

## 📌 Quick Navigation

- [프로젝트 개요](#프로젝트-개요)
- [현재 상태](#현재-상태)
- [완료된 과제 Phase 1-7](#완료된-과제-phase-1-7)
- [진행 예정 과제](#진행-예정-과제)
- [주요 기술적 성과](#주요-기술적-성과)
- [참조 문서](#참조-문서)

---

## 🎯 프로젝트 개요

### 프로젝트 비전

**ChartInsight Studio DataPipeline**은 **TRADING LAB 서비스**의 심장부입니다. 

이 프로젝트는 매일 아침, 시장이 열리기 전에 전날의 모든 주식 데이터를 수집하고, 분석하고, 가공하여, 투자자가 '오늘 주목해야 할 종목'을 찾을 수 있도록 돕는 자동화된 데이터 파이프라인입니다.

### 핵심 목표

우리의 최종 목표는 **'Stage 1: 퀀트 기반 주도주 스크리닝'**에 필요한 모든 데이터를 `daily_analysis_results` 테이블에 안정적으로 준비하는 것입니다.

이 테이블에는 각 종목의:
- **상대강도(RS) 점수**: 시장 대비, 업종 대비 얼마나 강한가?
- **재무 등급**: 기업의 재무 건전성은 어떤가?
- **기술적 지표**: 차트상 매매 타이밍은 적절한가?

이 모든 정보가 매일 자동으로 계산되어 저장됩니다.

---

## 🎯 현재 상태 (2025-10-31 기준)

### 프로젝트 성숙도

```
기반 구축 ████████████████████ 100%
핵심 기능 ████████████████████ 100%
안정화   ████████████████████ 100%
최적화   ██████████████░░░░░░  70%
```

### 최신 버전

- **DataPipeline 버전**: v7.1
- **마지막 주요 성과**: SIMULATION 모드 v7 아키텍처 완성 (2025-10-30)
- **운영 상태**: ✅ 프로덕션 준비 완료

### 완료된 주요 마일스톤

| 성과 | 완료일 | 상세 문서 |
|------|--------|----------|
| **SIMULATION v7 아키텍처** | 2025-10-30 | `report_test_simulation_architechture.md` |
| **P2 스케줄링 안정화** | 2025-10-30 | `report_과거실행자동트리거.md` |
| **DART API 최적화 v3.8** | 2025-10-26 | `DART_API_Optimization_Final_Report_v3.8.md` |
| **RS 점수 계산 기능** | 2025-10-22 | `RS_SCORE_IMPLEMENTATION_REPORT.md` |

### 다음 우선순위 과제

1. **제로 필터 통합** (P1) - 운영 효율성 70% 개선 예상
2. **선택적 업종 데이터 수집** (P2) - API 호출 80% 절감 예상
3. **테마 분석 통합** (P3) - 키움증권 테마 RS 분석 파이프라인 구축

---

## 📚 완료된 과제 (Phase 1-7)

### Phase 1-4: 기반 구축 및 핵심 기능 완성 (2025-10-13 ~ 10-26)

이 단계는 DataPipeline의 뼈대를 세우고, 데이터가 흐르는 통로를 만든 시기입니다. 마치 집을 짓기 위해 기초 공사를 하고, 수도관과 전기선을 연결하는 것과 같습니다.

#### Phase 1: 데이터 모델 및 분석 모듈 구축

**기간**: 2025-10-13  
**목표**: 데이터베이스에 분석 결과를 저장할 '그릇'을 만들고, 분석을 수행할 '도구'를 준비했습니다.

##### 완료된 작업

**1. `daily_analysis_results` 테이블 설계**
- **무엇을 했나요?**: 매일의 분석 결과를 저장할 데이터베이스 테이블을 만들었습니다.
- **왜 중요한가요?**: 이 테이블이 없으면 아무리 분석을 잘해도 결과를 저장할 곳이 없습니다. 마치 물건을 담을 상자가 없는 것과 같습니다.
- **어떻게 검증했나요?**: 실제로 데이터를 저장해보고, PostgreSQL에서 쿼리해서 확인했습니다.

**2. 핵심 분석 모듈 제작**
- **RS 계산기** (`rs_calculator.py`): 종목의 상대강도를 계산합니다.
- **재무 분석기** (`financial_analyzer.py`): 기업의 재무제표를 분석합니다.
- **기술적 분석기** (`technical_analyzer.py`): 차트 패턴과 지표를 분석합니다.

이 3개의 모듈은 `src/analysis/` 폴더에 독립적으로 존재하며, DAG에서 필요할 때 호출됩니다.

#### Phase 2: DAG 아키텍처 설계

**기간**: 2025-10-13  
**목표**: Airflow DAG들의 역할을 명확히 정의하고, 환경 설정을 체계화했습니다.

##### 완료된 작업

**1. 환경 변수 관리 체계 수립**
- **문제**: 로컬 개발과 Docker 환경에서 서로 다른 설정이 필요했습니다.
- **해결**: `.env.local` (개발용)과 `.env.docker` (운영용)로 분리했습니다.
- **효과**: 개발자는 로컬에서 편하게 테스트하고, 운영 환경은 안전하게 격리됩니다.

**2. DAG 역할 명확화**
- `dag_initial_loader`: 시스템 초기화 (처음 한 번만 실행)
- `dag_daily_batch`: 매일 증분 업데이트 및 분석
- `dag_live_collectors`: 고빈도 분봉 데이터 수집 (5분, 30분, 1시간)

각 DAG가 자신의 역할만 수행하도록 책임을 분리했습니다.

#### Phase 3: 배치 분석 파이프라인 구현

**기간**: 2025-10-13  
**목표**: `dag_daily_batch`의 모든 Task를 구현하고, 데이터가 끊김없이 흐르도록 연결했습니다.

##### 완료된 작업

**1. 데이터 수집 → 필터링 → 분석 → 저장 흐름 구축**

```
sync_stock_master (종목 마스터 동기화)
    ↓
update_low_frequency_ohlcv (일/주/월봉 데이터 수집)
    ↓
filter_analysis_targets (분석 대상 선정)
    ↓
[RS 계산 | 재무 분석 | 기술적 분석] (병렬 실행)
    ↓
load_final_results (DB에 최종 결과 저장)
```

**2. XCom을 통한 데이터 전달**
- **XCom이란?**: Airflow에서 Task 간에 데이터를 주고받는 메커니즘입니다.
- **왜 사용했나요?**: 각 Task가 독립적으로 실행되면서도, 이전 Task의 결과를 사용할 수 있게 합니다.

**3. 테스트 효율성 확보**
- **문제**: 1,400개 종목 전체를 테스트하면 3시간 이상 소요됩니다.
- **해결**: `target_stock_codes` 파라미터를 추가해, 소수 종목만 빠르게 테스트할 수 있게 했습니다.
- **효과**: 개발-테스트 사이클이 3시간 → 5분으로 단축되었습니다.

#### Phase 4: 통합 테스트 및 버그 수정

**기간**: 2025-10-13 ~ 10-26  
**목표**: 전체 파이프라인을 실제로 실행하면서 발견된 모든 문제를 해결했습니다.

##### 주요 해결 이슈

**1. 데이터 중복 적재 결함 ✅**
- **문제**: 같은 날짜에 DAG를 여러 번 실행하면 데이터가 중복으로 쌓였습니다.
- **원인**: `analysis_date` 컬럼이 `DateTime` 타입이어서, 실행 시간(초 단위)까지 다르면 별개 데이터로 인식했습니다.
- **해결**: `Date` 타입으로 변경해서, 같은 날짜는 항상 덮어쓰도록 했습니다.
- **중요성**: 파이프라인의 **멱등성**(여러 번 실행해도 결과가 같음)을 보장합니다.

**2. 필터 제로 책임 분리 ✅**
- **문제**: 종목 마스터 동기화 과정에서 ETF/ETN을 필터링하다가, 정상 종목을 '상장폐지'로 오인하는 심각한 버그가 발생했습니다.
- **원인**: 동기화 로직과 필터링 로직이 섞여 있었습니다.
- **해결**: 
  - 동기화는 순수하게 API 데이터를 DB에 반영만 합니다.
  - 필터링은 DAG 레벨에서 분석 대상을 선정할 때만 적용합니다.
- **교훈**: **단일 책임 원칙**(하나의 함수는 하나의 일만 한다)의 중요성을 배웠습니다.

**3. DART 기업 코드 매핑 로직 구현 ✅**
- **문제**: 재무 분석을 위해 DART API를 호출했는데, 종목코드와 기업코드가 매칭이 안 돼서 분석이 전부 스킵되었습니다.
- **원인**: 종목코드 `'5930'`와 DART 기업코드 `'00593000'`의 자릿수 차이를 처리하지 않았습니다.
- **해결**: `zfill(6)`, `zfill(8)` 함수로 자릿수를 맞춰주는 정규화 로직을 추가했습니다.
- **결과**: 재무 분석이 정상적으로 작동하기 시작했습니다.

**4. DAG 의존성 설정 오류 (Race Condition) ✅**
- **문제**: 종목 마스터를 업데이트하는 Task와 읽는 Task가 동시에 실행되어, 데이터 일관성이 깨졌습니다.
- **해결**: Task 간 의존성(`>>` 연산자)을 명시적으로 설정해서, 순서를 보장했습니다.
- **교훈**: 병렬 처리는 빠르지만, 데이터 의존성은 반드시 체크해야 합니다.

---

### Phase 5: RS 점수 계산 기능 완성 (2025-10-22)

이 단계는 DataPipeline의 핵심 가치를 만들어낸 시기입니다. '상대강도'라는 개념을 코드로 구현하고, 실제로 계산이 작동하는지 검증했습니다.

#### 목표

종목의 **시장 대비 상대강도**와 **업종 대비 상대강도**를 계산하여, 어떤 종목이 현재 '강한' 종목인지 객관적으로 판단할 수 있는 지표를 제공합니다.

#### 핵심 개념 설명

**상대강도(RS)란?**
- 단순히 "가격이 올랐다"가 아니라, "시장보다 더 많이 올랐다"를 측정합니다.
- 예: 삼성전자가 10% 올랐는데 KOSPI가 5% 올랐다면, 삼성전자의 시장 대비 RS는 양수입니다.
- 왜 중요한가? → 시장 전체가 떨어질 때도 강한 종목은 상대적으로 덜 떨어집니다.

#### 구현 단계

**1. 데이터베이스 스키마 확장**
- `sectors` 테이블 추가: 업종 정보 저장
- `stocks.sector_code` 컬럼 추가: 각 종목이 어떤 업종에 속하는지 매핑

**2. 업종 마스터 수집 DAG 신설**
- `dag_sector_master_update`: 주간 스케줄로 키움증권 API에서 업종 정보를 가져옵니다.
- 총 65개 업종(KOSPI 20개 + KOSDAQ 45개) 데이터를 자동으로 최신화합니다.

**3. 종목-업종 매핑 로직**
- **문제**: API에서 제공하는 업종명이 종목별로 표기가 다릅니다 (예: "반도체" vs "반도체 장비").
- **해결**: **Fuzzy Matching** 알고리즘을 사용해서, 유사도가 높은 업종을 자동으로 찾아 매핑합니다.
- **성공률**: 90% 이상의 종목이 정확한 업종에 매핑되었습니다.

**4. RS 계산 엔진 구현**
- `rs_calculator.py`가 월봉 데이터를 기반으로 RS를 계산합니다.
- **시장 RS**: 종목 vs KOSPI(001) 또는 KOSDAQ(101)
- **업종 RS**: 종목 vs 해당 업종 지수

#### 검증 결과

- ✅ 모든 종목의 `market_rs_score`가 정상 계산됨
- ✅ 매핑된 종목의 `sector_rs_score`가 정상 계산됨
- ✅ `daily_analysis_results` 테이블에 RS 값이 매일 기록됨

**참조 문서**: `RS_SCORE_IMPLEMENTATION_REPORT.md`

---

### Phase 6: DART API 최적화 (2025-10-26)

이 단계는 외부 API의 한계를 극복하고, 시스템의 안정성을 확보한 시기입니다.

#### 문제 상황

**재무 분석이 작동하지 않았습니다.**

- DART API 일일 호출 한도: 20,000회
- 실제 필요 호출: 42,000회/실행 (한도의 210%)
- 결과: API 에러로 DAG 실행 실패, 재무 분석 데이터 업데이트 불가

**정확도도 0%였습니다.**

- YoY 성장률이 전부 0.0%로 계산됨
- 재무 등급이 전부 'Fail'로 판정됨
- 원인: 데이터는 수집되었지만, 계산 로직에 버그가 있었습니다

#### 해결 방안

**1. 지능형 증분 수집 시스템**

기존 방식: 모든 종목에 대해 매번 6년치 데이터를 전부 수집  
→ 개선: 종목별로 필요한 만큼만 수집

```
신규 종목 (DB에 재무 이력 없음)
→ 5년치 수집 (YoY 계산을 위해 충분한 히스토리 필요)

기존 종목 (DB에 최근 데이터 있음)
→ 현재 연도 + 직전 1년만 수집 (YoY 계산용)
```

**2. API 호출 최적화 전략**

- **현재 연도**: 가장 최근 발행된 보고서 1개만 (1회)
- **직전 1년**: 전체 분기 보고서 (YoY 계산용, 4회)
- **그 이전**: 사업보고서만 (연간 데이터, 1회/년)

**3. 계산 로직 버그 수정**

- NumPy 타입 오류: `np.float64` → Python `float` 명시적 변환
- YoY 계산 오류: MultiIndex 정렬 로직 수정
- 주식총수 fallback: DART API 3단계 조회 시스템 구축

#### 최종 성과

| 지표 | 개선 전 | 개선 후 | 개선도 |
|------|---------|---------|--------|
| **API 호출 횟수** | 42,000회 | 11,200회 | **73% 절감** |
| **YoY 계산 정확도** | 0% | 100% | **완벽 달성** |
| **재무 등급 정확도** | 0% | 100% | **완벽 달성** |

**참조 문서**: `DART_API_Optimization_Final_Report_v3.8.md`

---

### Phase 7: SIMULATION 모드 v7 아키텍처 완성 (2025-10-30)

이 단계는 DataPipeline을 '실전용'과 '연습용'으로 분리하여, 안전하게 전략을 테스트할 수 있는 환경을 만든 시기입니다.

#### 왜 SIMULATION 모드가 필요한가?

투자 전략을 실제 돈으로 바로 테스트하는 것은 위험합니다. 과거 데이터로 먼저 시뮬레이션해서, "이 전략이 2024년에 사용했다면 수익이 났을까?"를 검증해야 합니다.

#### 핵심 아이디어: 시간 여행

```
LIVE 모드: 오늘(2025-10-31) 데이터 분석
SIMULATION 모드: 과거(2025-08-16) 시점으로 돌아가서 분석
```

SIMULATION 모드는 마치 타임머신을 타고 과거로 가서, 그 당시에 알 수 있었던 정보만 사용해서 분석하는 것입니다.

#### v7 아키텍처의 혁신: 완전한 역할 분리

이전 버전(v6)의 문제점:
- `dag_initial_loader`가 캔들 데이터도 준비하고, 재무 데이터도 준비했습니다.
- 역할이 불명확해서, 어떤 DAG가 무엇을 책임지는지 헷갈렸습니다.

v7의 해결책: **단일 책임 원칙(SRP)**

```
dag_initial_loader
→ 역할: 캔들 데이터 스냅샷만 생성
→ 결과물: simulation.candles 테이블

dag_financials_update
→ 역할: 재무 데이터 스냅샷만 생성
→ 결과물: simulation.financial_analysis_results 테이블

dag_daily_batch
→ 역할: 두 스냅샷을 읽어서 분석만 수행
→ 결과물: simulation.daily_analysis_results 테이블
```

각 DAG가 자신의 결과물만 책임지므로, 테스트와 디버깅이 훨씬 쉬워졌습니다.

#### 주요 기능

**1. 자동 감지 시스템**
- `target_datetime` 파라미터 생략 시 → Airflow Variable에서 자동으로 읽어옴
- `test_stock_codes` 파라미터 생략 시 → Variable에 저장된 종목 리스트 사용
- 효과: 매번 파라미터를 입력하지 않아도 됩니다.

**2. Look-Ahead Bias 완전 제거**
- **Look-Ahead Bias란?**: 미래 정보를 사용해서 과거를 분석하는 오류
- **예시**: 2024년 8월 분석인데 2024년 12월 재무제표를 사용하면 부정확합니다.
- **해결**: `target_datetime` 이전 데이터만 사용하도록 시점 필터링

**3. P2 스케줄링 안정화**
- **문제**: DAG를 unpause하면 290일 전부터의 과거 실행이 자동으로 트리거되었습니다.
- **해결**: `start_date=pendulum.now().subtract(hours=1)` 설정으로 과거 실행 방지
- **효과**: API 호출 99.7% 절감, 실행 시간 98% 단축

#### End-to-End 검증 (2025-10-30)

**테스트 시나리오**:
1. LIVE 모드로 초기 데이터 적재 (12개 종목)
2. SIMULATION 스냅샷 생성 (target_datetime: 2025-08-01)
3. SIMULATION 모드로 분석 실행

**검증 결과**:
- ✅ `simulation.candles`: 160,592개 데이터 생성
- ✅ `simulation.financial_analysis_results`: 12개 종목 재무 데이터
- ✅ `simulation.daily_analysis_results`: 11개 종목 분석 완료
  (207940 Samsung Biologics는 거래정지 상태로 Filter Zero에 의해 자동 제외)

**참조 문서**: 
- `report_test_simulation_architechture.md`
- `report_과거실행자동트리거.md`

---

## 🔮 진행 예정 과제

### 과제 8: 제로 필터 통합 (우선순위: P1)

#### 목표

현재 `dag_initial_loader`는 시장의 모든 종목(4,000개 이상)을 수집합니다. 하지만 실제로 분석하는 종목은 Filter Zero를 통과한 ~1,300개입니다.

**불필요한 2,700개 종목의 데이터를 수집하고 있습니다.**

#### 개선 방안

초기 적재 단계에서부터 Filter Zero를 적용하여, 분석 대상 종목만 수집하도록 통합합니다.

```python
# 현재 방식
dag_initial_loader → 4,000개 전부 수집
dag_daily_batch → Filter Zero 적용 → 1,300개만 분석

# 개선 방식
dag_initial_loader → Filter Zero 적용 → 1,300개만 수집
dag_daily_batch → 동일한 1,300개 분석
```

#### 예상 효과

- **API 호출 70% 절감**: 4,000개 → 1,300개 처리
- **처리 시간 단축**: 불필요한 데이터 수집/저장 제거
- **일관성 향상**: 두 DAG가 동일한 필터링 기준 사용

#### 구현 계획

1. `_determine_target_stock_codes` 태스크를 독립 함수로 분리
2. `dag_initial_loader`에서 재사용
3. SIMULATION 모드에서도 동일하게 적용

**참조**: `SIMULATION 모드 아키텍처 동기화 계획서.md` - 향후 과제 섹션

---

### 과제 9: 선택적 업종 데이터 수집 (우선순위: P2)

#### 목표

현재 `dag_initial_loader`는 65개 전체 업종의 데이터를 수집합니다. 하지만 12개 테스트 종목이 속한 업종은 2-3개에 불과합니다.

**필요 없는 62개 업종 데이터를 수집하고 있습니다.**

#### 개선 방안

테스트 종목이 속한 업종 코드만 동적으로 조회하여, 필요한 업종 데이터만 선택적으로 수집합니다.

```python
# 현재 방식
all_sector_codes = {모든 65개 업종}
market_index_codes = {'001', '101'}
all_necessary_codes = user_stocks + all_sector_codes + market_index_codes
# → 12개 + 65개 + 2개 = 79개 코드 처리

# 개선 방식
user_sector_codes = {테스트 종목이 속한 2-3개 업종만}
market_index_codes = {'001', '101'}  # KOSPI, KOSDAQ는 필수
all_necessary_codes = user_stocks + user_sector_codes + market_index_codes
# → 12개 + 2-3개 + 2개 = 16-17개 코드 처리
```

#### 예상 효과

- **API 호출 80% 절감**: 79개 → 16-17개 코드 처리
- **저장 공간 절약**: 불필요한 업종 캔들 데이터 미저장
- **유연성**: 종목 변경 시 필요한 업종만 자동으로 조정

#### 구현 계획

1. `_get_necessary_sector_codes()` 함수 구현
2. 배치 처리 로직 추가 (종목 500개씩 조회)
3. Fallback 메커니즘 (조회 실패 시 기본 업종 세트 사용)

**참조**: `SIMULATION 모드 아키텍처 동기화 계획서.md` - 향후 과제 섹션

---

### 과제 10: 테마 분석 통합 (우선순위: P3)

#### 목표

키움증권 API의 '테마' 기능을 활용하여, 테마 단위의 상대강도 분석을 DataPipeline에 통합합니다.

#### 테마 분석이란?

- **테마**: 유사한 특성을 가진 종목 그룹 (예: 'AI', '2차전지', '메타버스')
- **테마 RS**: 특정 테마가 시장 대비 얼마나 강한가를 측정
- **활용**: 강한 테마에 속한 종목을 우선적으로 선별

#### 현재 상태

테마 분석 기능은 **별도 프로토타입**으로만 존재합니다:
- `collect_all_thema_stocks.py`: 모든 테마와 구성 종목 수집
- `thema_rs_analyzer_v2.py`: 테마별 상대강도 계산 및 대시보드

이 기능들은 DataPipeline과 독립적으로 작동하며, Airflow DAG로 통합되지 않았습니다.

#### 통합 방안 (하이브리드 접근법)

**1. 데이터 수집 DAG 신설**
```
dag_thema_data_collector (주간 스케줄)
→ 키움 API에서 테마 그룹 및 구성 종목 정보 수집
→ live.thema_groups 테이블 저장
→ live.thema_stocks 테이블 저장 (테마-종목 매핑)
```

**2. 분석 파이프라인 확장**
```
dag_daily_batch에 테마 RS 분석 태스크 추가
→ calculate_thema_rs_task
→ 결과: live.thema_analysis_results 테이블 저장
```

**3. SIMULATION 모드 지원**
- 테마 데이터도 스냅샷 생성
- 과거 시점의 테마 RS 백테스팅 가능

#### 예상 효과

- **투자 전략 다양화**: 개별 종목 + 테마 기반 분석 결합
- **트렌드 파악**: 시장에서 주목받는 테마 실시간 추적
- **백테스팅 강화**: 테마 단위 투자 전략 검증 가능

#### 구현 계획

1. 프로토타입 코드 검토 및 리팩토링
2. 테이블 스키마 설계 (`thema_groups`, `thema_stocks`, `thema_analysis_results`)
3. DAG 구현 및 통합 테스트
4. 대시보드 기능 백엔드 API 연동

**참조**: 
- `backend/_temp_integration/chart_pattern_analyzer_kiwoom/collect_all_thema_stocks.py`
- `backend/_temp_integration/chart_pattern_analyzer_kiwoom/thema_rs_analyzer_v2.py`

---

## 🏆 주요 기술적 성과

### 1. 아키텍처 원칙 확립

#### 단일 책임 원칙 (SRP)
각 모듈, 각 함수, 각 DAG가 하나의 명확한 책임만 가지도록 설계했습니다.

**나쁜 예**:
```python
def sync_and_filter_stocks():
    # 동기화도 하고, 필터링도 하고... (책임이 2개)
    pass
```

**좋은 예**:
```python
def sync_stock_master():
    # 동기화만 함
    pass

def apply_filter_zero(stocks):
    # 필터링만 함
    pass
```

#### 멱등성 (Idempotency)
같은 작업을 여러 번 실행해도 결과가 동일하도록 보장합니다.

**예시**:
- `init_db()`: 10번 실행해도 테이블 구조는 항상 동일
- `dag_daily_batch`: 같은 날짜로 3번 실행해도 분석 결과는 1건만 저장 (UPSERT 사용)

### 2. 데이터 무결성 보장

#### Look-Ahead Bias 제거
시뮬레이션에서 미래 정보를 사용하는 오류를 완전히 차단했습니다.

```python
# 2025-08-16 시점 분석이라면
filtered_data = data[data['timestamp'] <= '2025-08-16 23:59:59']
# 이후 데이터는 절대 사용하지 않음
```

#### 타임존 일관성
모든 날짜/시간 데이터를 `Asia/Seoul` 타임존으로 통일했습니다.

### 3. API 최적화 전략

#### 지능형 증분 수집
신규 종목과 기존 종목을 구분하여, 필요한 만큼만 API를 호출합니다.

#### 3-Tier Fallback
데이터 조회 시 1차, 2차, 3차 소스를 순차적으로 시도하여 신뢰성을 확보했습니다.

```python
# 주식총수 조회 예시
1차: DART API istc_totqy 필드
2차: DART API distb_stock_co 필드
3차: DB Stock.list_count 컬럼
```

### 4. 테스트 효율성

#### 게이트키퍼 아키텍처
단일 진실 공급원 패턴으로, 모든 분석 Task가 동일한 종목 리스트를 사용하도록 보장했습니다.

#### 빠른 개발-테스트 사이클
`target_stock_codes` 파라미터로 소수 종목만 테스트하여, 개발 속도를 극대화했습니다.

---

## 📎 참조 문서

### 현재 활성 문서

| 문서 | 목적 | 최종 업데이트 |
|------|------|--------------|
| **DataPipeline_Project_Roadmap.md (본 문서)** | 프로젝트 전체 로드맵 | 2025-10-31 |
| **DART_API_Optimization_Final_Report_v3.8.md** | DART API 최적화 상세 | 2025-10-26 |
| **RS_SCORE_IMPLEMENTATION_REPORT.md** | RS 계산 기능 구현 | 2025-10-22 |
| **report_test_simulation_architechture.md** | SIMULATION v7 검증 | 2025-10-30 |
| **report_과거실행자동트리거.md** | P2 스케줄링 해결 | 2025-10-30 |

### 보관 문서 (Phase 1-4 상세)

Phase 1-4의 상세한 계획 및 실행 이력은 다음 문서들에 보관되어 있습니다:

```
docs/archive/phase1_to_phase4/
├── DataPipeline_Upgrade_Plan.md (초기 구축 계획)
├── DataPipeline_Upgrade_Report.md (구현 검증 결과)
├── DataPipeline_Improvement_Plan.md (개선 계획)
└── DataPipeline_Improvement_Points.md (문제점 분석)
```

**⚠️ 주의**: 보관 문서는 2025-10-13 ~ 10-26 시점의 역사적 기록입니다. 최신 정보는 본 문서를 참조하세요.

---

## 🎓 학습 포인트

이 프로젝트를 진행하면서 배운 핵심 교훈들입니다.

### 1. 설계의 중요성

**"급하게 가려면 천천히 가라"**

초반에 아키텍처를 명확히 정의하지 않으면, 나중에 리팩토링하는 데 훨씬 더 많은 시간이 듭니다. Phase 1-4에서 기반을 탄탄히 다진 덕분에, 이후 SIMULATION 모드와 API 최적화가 빠르게 완성되었습니다.

### 2. 작은 단위로 테스트

**"한 번에 하나씩"**

1,400개 종목 전체를 테스트하면 문제가 발생했을 때 원인을 찾기 어렵습니다. `target_stock_codes`로 5개 종목만 테스트하면, 버그를 빠르게 발견하고 수정할 수 있습니다.

### 3. 문서화의 힘

**"6개월 후의 나는 타인이다"**

지금은 코드가 명확해 보여도, 나중에 보면 "왜 이렇게 했지?"라는 의문이 듭니다. 상세한 문서와 주석은 미래의 나(또는 팀원)를 위한 선물입니다.

### 4. 외부 API는 믿지 말 것

**"API는 언제든 바뀔 수 있다"**

DART API 한도 초과 문제처럼, 외부 의존성은 언제든 문제를 일으킬 수 있습니다. Fallback 메커니즘과 로컬 캐싱으로 안정성을 확보해야 합니다.

---

## 📊 프로젝트 통계

### 코드 규모
- **DAG 파일**: 5개 (initial_loader, daily_batch, sector_master_update, financials_update, live_collectors)
- **분석 모듈**: 3개 (rs_calculator, financial_analyzer, technical_analyzer)
- **유틸리티**: 10+ 개 (filters, sector_mapper, logging_kst 등)

### 데이터 규모 (1,400개 종목 기준)
- **초기 적재**: ~20만 개 캔들 데이터
- **일일 증분**: ~5,000개 새 캔들
- **재무 데이터**: 종목당 평균 20개 분기 보고서

### 성능 지표
- **초기 적재 시간**: ~30분 (4,000개 종목 기준)
- **일일 배치 시간**: ~10분 (1,400개 종목 기준)
- **API 호출 효율**: 73% 절감 달성

---

## 🎯 마무리

DataPipeline 프로젝트는 이제 **프로덕션 준비 완료** 상태입니다.

매일 아침, 이 파이프라인은 자동으로:
1. 전날의 모든 주식 데이터를 수집하고
2. 1,400개 종목의 상대강도를 계산하고
3. 재무 등급을 판정하고
4. 기술적 지표를 분석하여
5. `daily_analysis_results` 테이블에 저장합니다

이 모든 과정이 사람의 개입 없이, **완전 자동**으로 작동합니다.

다음 단계는 이 파이프라인을 더욱 효율적으로 만들고(과제 8-9), 새로운 분석 기능을 추가하는 것(과제 10)입니다.

**우리는 데이터 파이프라인의 기반을 완성했습니다. 이제 그 위에 더 많은 가치를 쌓아갈 시간입니다.** 🚀

---

**문서 버전 이력**
- v1.0 (2025-10-13): 초기 작성
- v1.1 (2025-10-26): Phase 1-5 완료 반영
- v2.0 (2025-10-31): Phase 1-4 통합, SIMULATION v7 및 P2 스케줄링 완료 반영, 향후 과제 재정의
