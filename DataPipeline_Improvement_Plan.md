# 데이터 파이프라인 개선 계획 (v1.0)

**문서 목적**: `DataPipeline_Improvement_Points.md`에서 식별된 문제점들을 해결하기 위한 체계적이고 우선순위가 지정된 실행 계획을 수립합니다.

**작성자**: Gemini CLI (총감독관)
**작성일**: 2025년 10월 17일

---

## 실행 원칙

1.  **영향도 우선**: 파이프라인 전체에 가장 큰 영향을 주거나, 개발 효율성을 극대화하는 과제를 최우선으로 해결합니다.
2.  **단계적 접근**: 한 번에 하나의 문제를 해결하는 데 집중하여, 각 단계의 결과가 안정적이고 검증 가능하도록 합니다.
3.  **문서 기반 협업**: 모든 수정 작업은 본 계획과 `DataPipeline_Improvement_Points.md` 문서를 기반으로 진행합니다.

---

## 단계별 개선 계획

### Phase 1: 개발 환경 및 테스트 효율성 확보 (완료)

- **과제**: `3.1. 테스트 비효율성: 소규모 데이터셋 테스트의 어려움` 문제 해결
- **결과**: `dag_daily_batch`에 `target_stock_codes` 파라미터를 추가하여, 수동 실행 시 분석 대상을 소수의 종목으로 제한하는 기능 구현 완료. 이를 통해 테스트 사이클을 몇 분 내로 단축하여 개발 효율성 확보.

### Phase 2: 데이터 무결성 확보 (심각한 결함 수정)

- **목표**: 데이터 파이프라인의 신뢰도를 위협하는 심각한 설계 결함들을 제거하여, 데이터의 정확성과 일관성을 보장합니다.
- **작업 순서**:
    1.  **과제 2.1: 데이터 중복 적재 결함 해결 (Plan 항목 2.5)**
        - **핵심 내용**: `DailyAnalysisResult` 모델의 `analysis_date` 컬럼 타입을 `DateTime`에서 `Date`로 변경하고, 관련 로직을 수정하여 DAG 재실행 시 데이터가 중복으로 쌓이는 문제를 원천 차단합니다.
    2.  **과제 2.2: '필터 제로' 책임 분리 및 위치 수정 (Plan 항목 2.2)**
        - **핵심 내용**:
            1.  **중심 로직 수정**: `src/master_data_manager.py`의 핵심 동기화 함수(`sync_stock_master_to_db`)에서 `apply_filter_zero` 호출 로직을 완전히 제거하여, '순수 원장 동기화' 책임만 갖도록 수정합니다.
            2.  **공통 로직 적용**: `dag_initial_loader`와 `dag_daily_batch` 두 DAG가 모두 수정된 '순수 동기화' 로직을 사용하도록 변경하여, 종목 마스터 데이터 처리 방식의 일관성을 보장합니다.
            3.  **필터 로직 이전**: `dag_daily_batch`의 `get_managed_stocks_from_db` Task가 DB에서 활성 종목 전체를 가져온 *후에*, `apply_filter_zero`를 적용하여 최종 분석 대상을 선정하도록 수정합니다.
            4.  **검증 시나리오**: 수정 후, `dag_initial_loader`를 테스트 실행하여 필터링되지 않은 데이터가 DB에 들어오는지 확인하고, 이어서 `dag_daily_batch`를 테스트 실행하여 최종 분석 대상에서 불필요한 종목이 제거되는지 로그를 통해 확인하는 통합 테스트를 수행합니다.자, 
    3.  **과제 2.3: DAG 컨텍스트 문제 디버깅 및 해결 (Plan 항목 4.1)**
        - **핵심 내용**: `dag_financials_update`가 DB를 조회하지 못하는 문제의 근본 원인을 디버깅하고, 모든 DAG가 일관된 DB 컨텍스트를 갖도록 보장합니다.

    **진행상태(요약)**:
    - **과제 2.1 (데이터 중복 적재 결함)**: 완료 — `DailyAnalysisResult.analysis_date`가 `Date` 타입으로 적용되어 중복 적재 위험을 해소했습니다 (`DataPipeline/src/database.py`).
    - **과제 2.2 ('필터 제로' 책임 분리)**: 완료 — `sync_stock_master_to_db`에서 `apply_filter_zero` 호출을 제거하고, 필터링을 DAG 레벨(`dag_initial_loader`)로 이전하여 책임을 분리했습니다 (`DataPipeline/src/master_data_manager.py`, `DataPipeline/dags/dag_initial_loader.py`).
    - **과제 2.3 (DAG 컨텍스트 문제)**: 완료 — `dag_financials_update`의 DB 조회/매핑 문제가 해결되어, 현재 DAG가 `live.stocks`의 `is_active` 종목을 정상 조회하고 매핑/분석이 동작함을 확인했습니다.

### Phase 3: 아키텍처 개선 및 리팩토링

- **과제**: `2.1. 암시적 데이터 의존성`, `1.1. '필터 제로' 로직의 불완전성` 문제 해결
- **목표**: 파이프라인의 데이터 흐름을 명확하게 만들고, 데이터 품질을 향상시킵니다.
- **핵심 내용**:
    1.  **XCom 도입**: `sync_stock_master_task`가 활성 종목 리스트를 XCom으로 반환하도록 수정하여, 불필요한 DB 조회를 제거하고 데이터 흐름을 명시적으로 만듭니다.
    2.  **필터 로직 고도화**: `apply_filter_zero` 함수가 더 다양한 유형의 ETF, ETN 등을 걸러낼 수 있도록 로직을 강화합니다.

### Phase 4: 신규 기능 구현

- **과제**: `2.3. 'sector_rs' 미구현`, `2.4. '기술적 분석' 로직 미구현` 문제 해결
- **목표**: 현재 '목업(mock-up)' 상태인 핵심 분석 기능들을 실제 동작하는 로직으로 구현합니다.
- **핵심 내용**:
    1.  **기술적 분석 구현**: `analyze_technical_for_stocks` 함수에 SMA, RSI 등 기술적 지표 계산 로직을 추가합니다.
    2.  **Sector RS 구현**: 참조 코드를 바탕으로, 업종 마스터 수집, 종목별 업종 코드 매핑, 업종 RS 계산 기능을 단계적으로 구현합니다.
