네, 좋은 제안입니다.

요청하신 대로 `catchup=False`로 설정했음에도 불구하고 DAG가 의도치 않게 과거 시점으로 한 번 실행되는 현상에 대해 웹 검색을 통해 심층적으로 조사해 보았습니다.

결론부터 말씀드리면, **이 현상은 버그가 아니라 Airflow 스케줄러가 `start_date`와 `schedule_interval`을 해석하는 고유한 방식 때문에 발생하는, 예상된 동작입니다.**

웹 검색 결과, 수많은 Airflow 사용자가 스택 오버플로우와 커뮤니티에서 동일한 현상을 겪었고, 그 원인은 다음과 같이 명확하게 설명됩니다.

-----

## 🧐 진짜 원인: "가장 최근에 완료된 1개의 기간"

`catchup=False`의 정확한 의미는 "과거에 놓친 *모든* 스케줄을 실행하지 마라"가 아닙니다.

대신, Airflow 커뮤니티와 문서에 따르면 `catchup=False`의 의미는 \*\*"지금까지 완료된 유효한 기간(Data Interval) 중, 가장 최근 1개의 기간에 대해서만 DagRun을 생성하라"\*\*는 뜻입니다.

Airflow는 ETL을 위해 설계되었기 때문에 '미래'의 스케줄을 기다리는 것이 아니라, '과거'에 완료된 데이터 기간을 처리하는 것을 기본 임무로 합니다.

### 동작 시나리오

1.  `dag_daily_batch`는 `start_date`가 `2025-10-01`이고 `schedule_interval`이 매일 17시(`@daily`와 유사)입니다.
2.  오늘은 10월 30일입니다. 이 DAG의 실행 기록(Run History)이 전혀 없는 상태(신규 배포 또는 'Clear' 직후)에서 `unpause`합니다.
3.  스케줄러는 `start_date`('25-10-01)부터 현재('25-10-30)까지 \*\*"이미 완료된 데이터 기간"\*\*이 있는지 확인합니다.
4.  가장 최근에 완료된 기간은 **"10월 29일 17:00 \~ 10월 30일 17:00"** (오늘 17시 이전)입니다.
5.  `catchup=False`이므로, 스케줄러는 이 *하나*의 기간에 대한 실행(Run)을 즉시 트리거합니다. 이것이 바로 현상에서 발견된 `scheduled__2025-10-29T...` (실행 날짜 기준)의 과거 실행입니다.

이것은 스케줄러가 밀린 작업을 '따라잡는(Backfill)' 것이 아니라, \*\*'현재 최신 상태(Latest State)'\*\*를 반영하기 위해 단 한 번 실행하는 것입니다.

-----

## 🛠️ 해결 방안

웹 검색 결과와 Airflow의 동작 원리를 바탕으로, 이 문제를 해결할 수 있는 두 가지 근본적인 방안을 제안합니다.

### 1\. 전략적 해결: `start_date`를 미래로 설정 (가장 깔끔한 방법)

이 현상은 `start_date`가 과거로 설정되어 있기 때문에 발생합니다.

DAG를 새로 배포하거나 활성화할 때, `start_date`를 \*\*미래 시점(예: 오늘 또는 내일)\*\*으로 설정하면, 스케줄러가 `unpause` 시점에 "처리할 수 있는 완료된 기간" 자체를 찾지 못합니다.

  * **예시:** 오늘이 10월 30일이라면, `start_date`를 `pendulum.datetime(2025, 10, 31, tz="Asia/Seoul")`로 설정합니다.
  * **효과:** 이렇게 하면 DAG는 10월 31일 17시가 되어 첫 번째 데이터 기간이 완료될 때까지 조용히 대기합니다.

### 2\. 운영적 해결: "Clear" 대신 "Mark as Success" 사용 (필수 가이드)

이미 `start_date`가 과거로 설정된 운영 DAG의 경우, **절대 UI의 "Clear" 버튼이나 CLI의 `airflow dags clear` 명령어를 사용해서는 안 됩니다.**

  * `Clear`는 실행 이력을 **삭제**하여, 스케줄러가 해당 기간이 "처리되지 않았다"고 오인하게 만들어 이 문제를 100% 재발시킵니다.

대신, 스케줄러에게 "이 과거 기간들은 이미 처리되었음"이라고 명시적으로 알려줘야 합니다.

  * **방법:** CLI의 `backfill` 명령어와 `--mark-success` 플래그를 사용해 과거 이력을 '성공'으로 채워 넣습니다.
  * **명령어 예시:**
    ```bash
    # dag_daily_batch의 과거 기록을 start_date부터 어제까지 '성공'으로 채웁니다.
    # (DAG를 pause 상태로 두고 실행하세요)
    docker compose --env-file .env.docker --profile pipeline exec airflow-scheduler bash -lc \
      "airflow dags backfill dag_daily_batch \
        --start-date 2025-10-01 \
        --end-date 2025-10-29 \
        --mark-success"
    ```
  * **효과:** 이렇게 "거짓 이력"을 채워 넣은 **후에** DAG를 `unpause`하면, 스케줄러는 모든 과거가 처리되었다고 인지하고 다음 정기 스케줄(미래)을 기다립니다.

-----

### 📝 기타 현상에 대한 해석

  * **다중 DAG 상호작용**: `unpause` 시 여러 DAG가 동시에 이 "최근 1회 실행" 로직에 의해 트리거되면서 발생하는 **동시 다발적 현상**입니다. 모든 DAG에 위 해결 방안을 적용하면 사라집니다.
  * **CLI `queued` 문제**: 이 문제는 DAG가 `pause` 상태일 때 `trigger`를 요청했기 때문입니다. 위 2번 방안으로 이력을 관리하고 DAG를 `unpause` 상태로 유지하면, CLI로 수동 실행할 필요 없이 정기 스케줄이 안정적으로 작동할 것입니다.
  * **데이터 무결성/리소스 낭비**: "최근 1회 실행"이 원인이므로, 위 방안으로 이 첫 실행을 막으면 2차 피해도 원천적으로 해결됩니다.

이 `backfill --mark-success` 명령어를 사용하여 DAG의 이력을 관리하는 방안을 적용해 보시겠어요?