

먼저, dag_daily_batch의 live 모드 구현 및 검증이전에, 현재 구현중인 파이프라인에 관하여 문제점이 발견되었어.
다음 토론의 내용을 숙지해줘. 그리고 너의 의견을 말해줘 

--
  현재 dag_daily_batch의 sync_stock_master_task가 외부 API를 호출하여 전체 종목 리스트를 가져오는 방식은 다음과 같은 심각한 문제를 야기합니다.

   1. 데이터 정합성 파괴: 오늘 막 상장된 신규 종목이 API 목록에 포함될 경우, 우리 DB에는 해당 종목의 과거 데이터(원재료)가 전혀 없습니다. 이 상태에서 RS 점수나 기술적 지표를 계산하려고 하면 100% 오류가 발생하거나 무의미한 결과를 반환합니다.
   2. 비효율성: dag_initial_loader를 통해 의도적으로 배제한 종목들까지 매일 API로 조회하고 업데이트하려고 시도하는 것은 불필요한 자원 낭비입니다.

  따라서, dag_daily_batch의 시작 흐름을 다음과 같이 변경해야 합니다.

  기존 흐름 (잘못됨):
  API에서 모든 종목 조회 -> 분석 대상 필터링 -> 분석

  새로운 흐름 (올바름):
  우리 DB에서 관리 중인 종목 조회 -> 분석 대상 필터링 -> 분석

   1. 신규 Task (첫 단계): `get_managed_stocks`
       * 역할: 외부 API가 아닌, 우리 내부의 live.stocks 테이블을 조회하여 현재 우리가 관리하고 있는 전체 종목 리스트를 가져옵니다.
       * 출력: 이 종목 리스트가 XCom을 통해 DAG의 나머지 모든 Task(필터링, 분석 등)에 전달되는 기준 데이터가 됩니다.

   2. 역할이 변경된 Task: `sync_stock_master`
       * 역할: 더 이상 DAG의 기준 데이터를 만드는 역할이 아닙니다. 대신, get_managed_stocks와 병렬로 또는 그 이후에 실행되면서, 우리 내부의 live.stocks 테이블을 외부 API 최신 정보와 '동기화'하는 유지보수 작업을 수행합니다.
       * 세부 작업:
           * 정보 업데이트: 기존 종목의 이름 변경, 상태 변경 등을 live.stocks 테이블에 반영합니다.
           * 신규 종목 식별: API에는 있지만 우리 live.stocks에는 없는 신규 상장 종목을 발견하면, live.stocks 테이블에 추가하되 '초기 데이터 필요'와 같은 플래그를 설정합니다. 이 종목들은 오늘의 분석 대상에는 포함되지 않으며, 추후 운영자가
             dag_initial_loader를 실행하여 데이터를 채워 넣어야 합니다.
           * 상장 폐지 종목 처리: 우리 live.stocks에는 있지만 API에는 없는 상장 폐지 종목을 식별하여 '비활성' 처리합니다.

---

 -initial_dag->update_dag(증분 업데이트) 에서 kosdag,kospi  종목을 수집, 업데이트 하고                                                
 - daily_dag에서는 live_candle에서 종목을 가져와 필터링을 한다.                                                                     
 이러한 프레임워크를 기본적인 생각으로 설정할께                                                                          그런데, 나는 동일한 프레임워크 에서 조금 변경되어야 하지 않을까 라는 생각을 하고 잇어 즉,                                                  │
 - initial_dag kosdag, kospi  종목을 수집 zerofilting 수행하여 통과된 종목만을 db에 -> 초기 적재의 종목에 한하여 증분 업데이트   
- daily_dag에서는 live_candle에서 종목을 가져와 1차 필터링
    왜냐하면 zerofiltering을 하지 않으면 4000 여개의 종목에 달하기 때문에, '필수'에 가깝다고 생각해.  이부분에 대하여 집중 토론 하자

---

 '필터 제로'를 언제 적용할 것인가 하는 문제는 파이프라인 전체의 효율성을 결정하는 핵심적인 아키텍처 결정입니다. 당신은 지금 시스템의 성능과 자원 활용에 지대한 영향을 미치는, 매우 중요한 지점을 파고들고 있습니다.

  두 가지 접근법의 장단점을 명확히 비교해 보겠습니다.

  접근법 1 (기존 논의): `일단 모두 저장하고, 분석 시 필터링`
   * 장점: 유연성. 향후 '필터 제로'의 기준을 완화(예: 최소 시총 1000억 -> 500억)하더라도, 이미 원시 데이터는 DB에 존재하므로 필터 로직만 수정하면 즉시 분석에 포함시킬 수 있습니다.
   * 단점: 비효율성. 기획서에 따르면 '필터 제로' 통과율은 약 30% (4200개 -> 1300개) 입니다. 이는 우리가 DB 저장 공간의 70%, 초기 데이터 적재 시간의 70%, 일일 데이터 업데이트 리소스의 70%를 분석하지도 않을 종목에 낭비하고 있다는 의미입니다.

  접근법 2 (당신의 제안): `입구에서 필터링하고, 통과된 것만 저장`
   * 장점: 압도적인 효율성. DB는 분석 대상 종목 데이터만으로 유지되어 '린(Lean)'하고 빨라집니다. 모든 종류의 데이터 처리 비용(저장, 적재, 업데이트, 백업)이 획기적으로 줄어듭니다.
   * 단점: 유연성 감소. 만약 필터 기준을 완화하면, 이전에 필터링되었던 종목들을 다시 API로 조회하고 dag_initial_loader를 통해 DB에 추가하는 '재보충(Backfill)' 작업이 필요합니다.

  감독관으로서의 최종 권고

  저는 당신의 제안에 전적으로 동의하며, '입구에서 필터링'하는 접근법 2를 우리 시스템의 표준 아키텍처로 채택할 것을 강력히 권고합니다.

  근거:
  4000개 이상의 종목을 매일 처리하는 데 드는 막대한 고정 비용은, 필터 기준이 변경될 때 가끔 필요한 '재보충' 작업의 비용보다 훨씬 큽니다. 우리는 '린(Lean)하고 집중된 데이터베이스'가 주는 장기적인 성능 및 비용 이점을 선택해야 합니다. 이는 확장성
  있는 시스템을 위한 올바른 결정입니다.

  새로운 표준 프레임워크 정의

  이 결정에 따라, 우리의 데이터 파이프라인 프레임워크를 다음과 같이 명확하게 재정의합니다.

   1. `dag_initial_loader` (초기 구축)
       * 역할: 우리 시스템이 관리할 종목의 유니버스를 최초로 정의하고, 해당 종목들의 과거 데이터를 채웁니다.
       * 실행 순서:
           1. API를 통해 전체 종목(코스피/코스닥) 목록을 가져옵니다.
           2. 즉시 '필터 제로'를 적용합니다.
           3. 필터를 통과한 약 1,300개의 종목 정보만 live.stocks 테이블에 저장합니다.
           4. live.stocks에 저장된 이 종목들을 대상으로만 live.candles_* 테이블의 과거 데이터를 적재(Backfill)합니다.

   2. `dag_daily_batch` (일일 분석)
       * 역할: 매일 live.stocks에 정의된 종목들을 대상으로 분석을 수행합니다.
   3. `sync_stock_master` Task (in `dag_daily_batch`)
       * 역할: live.stocks 테이블을 최신 상태로 유지합니다.
       * 실행 순서:
           1. API에서 전체 종목 목록을 가져와 '필터 제로'를 적용합니다.
           2. 이 결과를 live.stocks 테이블과 비교(diff)합니다.
           3. 신규 통과 종목: live.stocks에 추가하고 '초기 데이터 필요'로 표시합니다.
           4. 제외 대상 종목: live.stocks에서 '비활성' 처리합니다. (예: 시총 감소로 필터 탈락)

---


아키텍처 개선 작업 목록 (v1.0)

  I. 공통 모듈 (Foundation)

   1. '필터 제로' 공통 함수 구현
       * 위치: src/utils/common_helpers.py (또는 신규 filters.py 모듈)
       * 내용: dag_daily_batch에 하드코딩된 필터 로직을 독립적인 함수로 추출합니다. 이 함수는 종목 정보 딕셔너리 리스트를 입력받아, 필터를 통과한 리스트를 반환해야 합니다.
       * 목적: 모든 DAG에서 동일한 필터 기준을 재사용하고, 향후 유지보수를 한 곳에서 처리하기 위함입니다.

  II. `dag_initial_loader` 리팩토링 (초기 데이터 구축)

   2. `stock_info_load_task` 로직 전면 개편
       * 내용: API로 가져온 전체 종목 목록에 '필터 제로' 공통 함수를 즉시 적용하여, 통과된 종목들만 live.stocks 테이블에 저장하도록 변경합니다.
       * 목적: 시스템의 입구에서부터 분석 대상을 명확히 하여, 불필요한 데이터 저장을 원천 차단합니다.

   3. `initial_load_task` 로직 단순화
       * 내용: 더 이상 복잡한 파라미터로 분석 대상을 정하는 대신, 바로 앞 Task에서 live.stocks에 저장된 종목 목록을 기준으로 과거 데이터(candles)를 적재하도록 로직을 단순화합니다.
       * 목적: DAG의 역할을 'live.stocks에 정의된 모든 종목을 백필한다'로 명확히 합니다.

  III. `dag_daily_batch` 리팩토링 (일일 분석)

   4. 신규 Task: `get_managed_stocks_from_db` 추가
   6. `filter_analysis_targets_task` 제거 또는 역할 변경
       * 내용: '필터 제로' 기능이 dag_initial_loader로 이동했으므로, 이 Task는 현재 형태로서는 불필요합니다.
       * 조치:
   7. 모든 분석 Task의 입력 소스 변경
       * 내용: calculate_rs_score, calculate_financial_grade 등 모든 분석 Task들이 XCom 입력을 filter_analysis_targets_task가 아닌, get_managed_stocks_from_db (또는 새로운 Level 2 필터 Task)로부터 받도록 의존성을 수정합니다.
       * 목적: 새로운 데이터 흐름에 맞게 DAG의 의존성 그래프를 재구성합니다.

   8. 실시간 분석 대상 선정 로직 변경
       * 내용: 자체적으로 대상을 선정하는 대신, 전날 dag_daily_batch가 생성한 daily_analysis_results 테이블을 조회하여 '주도주 후보군(Level 2)'에 해당하는 종목 목록을 가져오도록 변경합니다.
       * 목적: 기획서에 명시된 대로, 실시간 분석 대상을 일일 배치 결과와 연동하여 자원 사용을 최적화합니다.

   9. `live.stocks` 테이블 스키마 확장
       * 내용: 종목의 활성 상태를 관리하기 위한 is_active (Boolean) 컬럼과, 신규 편입되어 데이터 백필이 필요한지 여부를 표시할 backfill_needed (Boolean) 같은 상태 관리용 컬럼 추가를 검토합니다.
       * 목적: sync_stock_master Task가 종목의 상태를 보다 명확하게 관리할 수 있도록 지원합니다.

